{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WangKun00293/bart-bert-project/blob/main/BERT_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "9cfc8818ff474089ba9e18f7a2fd3c8c",
            "406866f7c11441c38442edcdbe0576f7",
            "db53f7b77ba74dcf84e39c2d508efb8e",
            "58dc4fbdcf064d5589a09e795233fc77",
            "43d051fcbfeb494883d706fd215c90a8",
            "3d3adda1ac7749be91e1d02dc00c32e6",
            "7c16b35e8bb44d50998e7241bf2610f4",
            "eef3666c09c04937bf1541f8f9d8e31c",
            "8838b2c705dd4b2fab2e73ceedcf69ae",
            "8f743b02f23247f090490b5a6556a89a",
            "bc812bb2c4d34f2ba93775094111959b",
            "97d27febd7244e448c6f2f4249933671",
            "ba6abae2147341a49e7837a12719c662",
            "fc9f6c852cd8457b978d592b15e88fc2",
            "5a2ee947f7484cbf84b074257797db67",
            "ceadb4c956e94de0bcdd3285c6e0ec14",
            "f2e1ad6ad3fa4aef82b6462008ad4278",
            "6369e12e2d124b9b811caea6ac1d7822",
            "dc29dda3fbe143f0b41c35746f530063",
            "e23b7f76b45c4153ae7c952d40360ee5",
            "630b20f5ad7b42339039329e93130b3e",
            "c4bc3ff58b434c22a2a5bda27917378a",
            "40f0daf0c77f4419ad29bb9d2aa23945",
            "dc8430733a134381a87a5b234f72d41e",
            "d849842eab244cc2a606fc2f25c88f3a",
            "4ec5ac8d113648acbe9bb279daef3d37",
            "a494c7cd7fd64a6383cef8e24b83f502",
            "e7c3b2eef721476fb73b47a8e1c8d687",
            "42b2f44ccb614adfb72369685308a8e4",
            "8ae014e52aa44605a92bf2ddecb004fe",
            "d5950f393eb24958bab26722b12760f1",
            "3d05099599bf43eda909f9933e5da4d8",
            "8579f07f94b446e1b96a7f954cb8a1d1",
            "770f7413452f4f8d9e299bacd4b3f097",
            "47fb251957e84f9b964a3aafdb78aeea",
            "451f688ed5304b739b6cba6f0b76fea2",
            "51a029d364e6413daade476b6cda9fe5",
            "b9f863542a3b4e99b2dcdbd91a7ef12b",
            "4dca1862aa3a4606ba226db7a06a616f",
            "4583294661fa4221a709b239ef9b7244",
            "39d5affa8877430bb4b42af243c9505f",
            "93475260bdf24ee0be6208a6d442943d",
            "9b93855aa1814069bdaaeac180e51508",
            "11f6eec2febb419e933b31c8cc8e4185",
            "44f403af911b45eeb1350c483b46dc81",
            "ae0229cb10e44eda8e7c726f37651429",
            "25fa467a93634cd58e60195507afce2a",
            "02a1543b2e924657b39f4308d215d233",
            "5fa5e02bb0604e999bb78763a5c3d0ec",
            "da2c1f6236ce4803b4df84a29fe96517",
            "de46bfe9360346f4bc1eadbdc7e6d7bb",
            "89307ca0f5de49e1aa20137198c7068c",
            "8bd1068874874b639b8e5b3c45019e65",
            "01db4fe260954c29b2ed0b83f1d5342c",
            "82bec5bffd80420d9637dadd9067785b"
          ]
        },
        "collapsed": true,
        "id": "6VPjsAg_29N_",
        "outputId": "02503ad4-2ada-4635-cd97-b49f997f1c4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cfc8818ff474089ba9e18f7a2fd3c8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/8.07k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97d27febd7244e448c6f2f4249933671",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40f0daf0c77f4419ad29bb9d2aa23945",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "770f7413452f4f8d9e299bacd4b3f097",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44f403af911b45eeb1350c483b46dc81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "ag_news = load_dataset(\"ag_news\")\n",
        "## load dataset\n",
        "import pandas as pd\n",
        "df_train=ag_news[\"train\"].to_pandas()\n",
        "test=ag_news[\"test\"].to_pandas()\n",
        "from sklearn.model_selection import train_test_split\n",
        "sample = df_train.sample(n=10000, random_state=1)\n",
        "train, val = train_test_split(sample, test_size=0.1, random_state=311)\n",
        "train_0=train[train[\"label\"]==0].sample(n=500, random_state=323)\n",
        "train_1=train[train[\"label\"]==1].sample(n=800, random_state=323)\n",
        "train_2=train[train[\"label\"]==2].sample(n=1200, random_state=323)\n",
        "train_3=train[train[\"label\"]==3].sample(n=1500, random_state=323)\n",
        "df_train=pd.concat([train_0,train_1,train_2,train_3]).sample(frac=1, random_state=324).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz2kfchJ3JFh",
        "outputId": "9a14d399-837a-41eb-dbb5-d1ebb82ce523"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5N1y6Xs4n9x"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7uVlScsKykI"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "    text = re.sub(r'[&\\\\@#\\$%\\^*()_+={}\\[\\]:;\"\\'<>\\|`~]', '', text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkFcbPsgK25m"
      },
      "outputs": [],
      "source": [
        "df_train['text'] = df_train['text'].apply(clean_text)\n",
        "test['text'] = test['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqUV5LMBLXHQ"
      },
      "outputs": [],
      "source": [
        "# don't need prompt for BERT, add it for fine-tuning BART\n",
        "label_map = {0: \"Write a World News\", 1: \"Write a Sport News\", 2: \"Write a Business News\", 3: \"Write a Science/Technology News\"}\n",
        "df_train['prompt'] = df_train['label'].map(label_map)\n",
        "df_remain, df_BART= train_test_split(df_train, test_size=0.5, random_state=325)\n",
        "df_test, df_val = train_test_split(test,test_size=0.5,random_state=325)\n",
        "df_remain=df_remain.reset_index(drop=True)\n",
        "df_BART=df_BART.reset_index(drop=True)\n",
        "df_test=df_test.reset_index(drop=True)\n",
        "df_val=df_val.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjYGwHMqGDCS"
      },
      "source": [
        "## Base-line model with BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "e7617ad338504c8a882364386d7f8495",
            "445a28224a7f4c6c947c7360d063e00c",
            "1dd3d34c15cb4c8abf81f12b0b70105b",
            "ebc557ceb1b142658981491f486a2049",
            "54b56c2e7b04426ea621b2fb731aae38",
            "92cef01c0322485fa7bba030893fc4e0",
            "f7ba43c69c154bbbb565f2ab4199b23b",
            "d1edec27bfd14307b4e4c1a21fbb061b",
            "0d614e4fae9f4508b901a14a39aeeed3",
            "107d4f4a16174704be74c908a209b9eb",
            "61d921810b334e5ba63f1136ef44d4ca",
            "180cfa84bfc642879f6acb655817d25d",
            "29601cf3eb3e40edbe153abeb17018d3",
            "f55271022ecb479b851fa8604154e73e",
            "ad38f9f3952e421c9f461723972b23f0",
            "6998d0118fae422e821d4bc717735f58",
            "3271ace0f161424d901d70a79508081e",
            "d405d70866cc4486955264eb6b4b6e23",
            "3c7fe567213e4d0cb1ef7e383d927c36",
            "035ab1e102724aaab27cf3293e413754",
            "9e516fd7e4a84d3794cda8d536f74541",
            "aa0e8cf3da2e4e448cf5b414f6771897",
            "f6f872e1a937424f9e075091a027719f",
            "8dea683d18ae45f798bd860ac89c28b7",
            "3bf69db48e64413488f92ab789d76d6d",
            "aa69eb2e8f6e4cc7b28a0e326fd14eae",
            "c5763a18612040e5ae53dff7848557a8",
            "1131bf7970a9472ab62874557f08dfcf",
            "c1c67d256ae246849822ea3628c4ef01",
            "c84610665e75419eae8fef5e4aaf2778",
            "57be9d6c51b94996889001c1729c595c",
            "5c4db6ba7ee244cc9d2ef57d8331b7f9",
            "16359d0dc73941bb903fd9a2be235385",
            "02d6217e53004f6eb268fd30347a8949",
            "5030eb11a9774d2fae75c53ffd1c7be6",
            "1dd023525af148999e06ae0199b9372b",
            "c600a4ea4bed464da68aaa6c762e7f41",
            "780b9e578b0c4b9b9a89e32555bf4ef4",
            "c70e687a91904226af519107a799b3c3",
            "ba1d22ac297c4968be9b7ba2b93d4e26",
            "818043cf72c44c74a53085589e2693b1",
            "b9a4408e91884594b0add7f305e6e57f",
            "c47b2561c4eb4fe8b14770419b9c51a2",
            "2a97730519ad469da1ea4051fe69eb4c",
            "072aba0d6024401781ec075f6e38a4a6",
            "53c4d202d1064e73a517d0deeb7d95d0",
            "0fa03c7471204e89bd05d86f10887ab8",
            "48a54a41847f4f629a357845597a3bf7",
            "c244234541ed4081b7d23f7b71379cb9",
            "bd006315ed28436e8a3a5d517c04c314",
            "73bc6ea9c3b142dd8c7124477af56a8a",
            "88e69263b5b44f23acd51947aebf431b",
            "c6a51fad4a80433eacff84ff576d8959",
            "d223e660717c44d19caabf5658002e18",
            "bb2263675d544c2885c3827282f4c43f"
          ]
        },
        "id": "9U7Od27tGCfw",
        "outputId": "5b086065-ffad-4866-dd2b-801571d5158e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7617ad338504c8a882364386d7f8495",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "180cfa84bfc642879f6acb655817d25d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6f872e1a937424f9e075091a027719f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02d6217e53004f6eb268fd30347a8949",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "072aba0d6024401781ec075f6e38a4a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvOoVI0-HZs8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = dataframe.label\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.targets[index], dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxW8AXNyHcMc"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 320\n",
        "BATCH_SIZE = 16\n",
        "train_dataset = TextDataset(dataframe=df_train, tokenizer=bert_tokenizer, max_len=MAX_LENGTH)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_dataset = TextDataset(dataframe=df_val, tokenizer=bert_tokenizer, max_len=MAX_LENGTH)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataset = TextDataset(dataframe=df_test, tokenizer=bert_tokenizer, max_len=MAX_LENGTH)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXPs-1KRHgbP",
        "outputId": "c526095f-e285-461f-cbbb-3b4e1e84a618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training Loss: 0.4864871038645506, Validation Loss: 0.35436977159638866\n",
            "Epoch: 2, Training Loss: 0.2045171178430319, Validation Loss: 0.325832781248859\n",
            "Epoch: 3, Training Loss: 0.11916310729086399, Validation Loss: 0.35660128549774406\n",
            "Epoch: 4, Training Loss: 0.06591740463860334, Validation Loss: 0.411454915129697\n",
            "Epoch: 5, Training Loss: 0.03962298171687871, Validation Loss: 0.4795967113146433\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bert_model.to(device)\n",
        "learning_rate = 2e-05\n",
        "optimizer = torch.optim.AdamW(params=bert_model.parameters(), lr=learning_rate)\n",
        "epochs = 5\n",
        "bert_model.train()\n",
        "import os\n",
        "\n",
        "# Directory where you want to save your models\n",
        "model_save_path = \"/content/drive/MyDrive/planB/bert\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    bert_model.train()\n",
        "    total_loss = 0\n",
        "    # Training loop\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation loop\n",
        "    bert_model.eval()  # Switch to evaluation mode\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_val_loss += loss.item()\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "    # Save model after each epoch\n",
        "    epoch_save_path = os.path.join(model_save_path, f'BERT402_epoch_{epoch+1}')\n",
        "    os.makedirs(epoch_save_path, exist_ok=True)\n",
        "    bert_model.save_pretrained(epoch_save_path)\n",
        "    bert_tokenizer.save_pretrained(epoch_save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmtwrIIi4ui5"
      },
      "source": [
        "# load model and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcfqvKpRmlny"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/planB/bert/BERT402_epoch_2')\n",
        "bert_model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/planB/bert/BERT402_epoch_2', num_labels=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyHe0zYkn-Az"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = dataframe.label\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.targets[index], dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H56ALMxoQSw"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 320\n",
        "BATCH_SIZE = 16\n",
        "test_dataset = TextDataset(dataframe=df_test, tokenizer=bert_tokenizer, max_len=MAX_LENGTH)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_dataset = TextDataset(dataframe=df_val, tokenizer=bert_tokenizer, max_len=MAX_LENGTH)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQOEm8URM7pM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dykgdukTbTD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()  # Put the model in evaluation mode\n",
        "\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "            logits = outputs.logits.detach().cpu().numpy()\n",
        "            label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "            predictions.extend(np.argmax(logits, axis=1).flatten())\n",
        "            true_labels.extend(label_ids.flatten())\n",
        "\n",
        "    avg_accuracy = accuracy_score(true_labels, predictions)\n",
        "    print(f'Validation Accuracy: {avg_accuracy}')\n",
        "\n",
        "    # Detailed classification report\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(true_labels, predictions, target_names=['Class1', 'Class2', 'Class3', 'Class4']))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(true_labels, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO-Gah_UTdyd",
        "outputId": "48462112-a782-458b-bba2-167eb8b1367d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8981578947368422\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      Class1       0.97      0.83      0.89       965\n",
            "      Class2       0.96      0.98      0.97       972\n",
            "      Class3       0.84      0.87      0.85       933\n",
            "      Class4       0.84      0.92      0.88       930\n",
            "\n",
            "    accuracy                           0.90      3800\n",
            "   macro avg       0.90      0.90      0.90      3800\n",
            "weighted avg       0.90      0.90      0.90      3800\n",
            "\n",
            "Confusion Matrix:\n",
            " [[800  31  79  55]\n",
            " [  6 949  11   6]\n",
            " [ 13   4 809 107]\n",
            " [ 10   4  61 855]]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bert_model.to(device)\n",
        "evaluate(bert_model, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7MoQ2q1F72e",
        "outputId": "3b439896-97ef-483b-bf47-de6812ba5e2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9078947368421053\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      Class1       0.96      0.87      0.91       935\n",
            "      Class2       0.97      0.98      0.98       928\n",
            "      Class3       0.86      0.86      0.86       967\n",
            "      Class4       0.85      0.92      0.88       970\n",
            "\n",
            "    accuracy                           0.91      3800\n",
            "   macro avg       0.91      0.91      0.91      3800\n",
            "weighted avg       0.91      0.91      0.91      3800\n",
            "\n",
            "Confusion Matrix:\n",
            " [[814  25  56  40]\n",
            " [  4 911   8   5]\n",
            " [ 22   0 832 113]\n",
            " [  8   2  67 893]]\n"
          ]
        }
      ],
      "source": [
        "evaluate(bert_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKfnvrTEWi5F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCQ22F8-W14s"
      },
      "source": [
        "## use test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "5995beface3d4c91a5ddb683eba54357",
            "b24013921e4640d3b65d178cd39de7d3",
            "3c061e57835d472c9b7016971ad35a85",
            "ee7da96ed42341acbf25ce3c62fd8cfa",
            "c39b6817e5f044a892bd89ebf5e7d0dd",
            "c552500886f44e09b1c7fe5db0b8988e",
            "a6d3d4c668714488bec0d1f5992a022a",
            "5a46375e10c34a85a3bd1dbd3fb67b40",
            "3f80fbc7810b4da3977aca18141dca91",
            "2451e97582a04315a92216044c5e0e05",
            "5b02995e9e2c4f9ca923bd93c3841e3f",
            "5863ca16d58d4fc58fc1d5a881a6ca1c",
            "03cdb7b49170412d817b7acce823ede5",
            "1e84dc4b576e448589b821ea5ece51f3",
            "2a2d8028fb924d90b7a9609f09d66ce8",
            "3be047b0ab314a9686241a1360050a2a",
            "1e2e182802f149318d6c5b7389a1117b",
            "e5886a63a2f7443fbaa6d81ee92aa857",
            "883874e4a68948439e76603b902d7893",
            "043ba909622f46bcac6dbc182426479a",
            "87a03d8d5b7c4074af2bf192dab1e5b7",
            "485f12255c4d4a1db260bede5370ef47",
            "f38a8ea837be4e2eb713ab512312c645",
            "4762d037c05f4934a01b357a1897621f",
            "c0fe0dcd571c42ba9fdc5d64847a48e8",
            "9d97cf0221264553bc88936a578448ac",
            "c4b2484f3d63458e94fffc1fb5b4b0c2",
            "1435c594ae154512bc4361162468e339",
            "3ef600dfc68e4dec8b5079f38675ce0f",
            "1f15c7a3b3724304b45e58bec3fdd542",
            "78917f5c010a48dd830d94b09bcef546",
            "f54feac1909c49bc9c4c65d02b4c6e75",
            "c7b6288fee5945e9a89fcaf17eba851d",
            "9a1a6a1900754c2a8c8f05ede6c8c95a",
            "76d94c3a4de44da2bc8cf4ddd0f73d0d",
            "3728762acff74afdaec9601949282fed",
            "cbf1217677874d4c90517c2bba8e7edc",
            "cf5690e153e04716976cd1c6c1a84c69",
            "f3f22e2692f547f1bc0fa3729876490a",
            "129c3346b2254299af6fef1aff0bbb6f",
            "a9920b843dea4562953120807a016578",
            "9196bacbf4b04704afdfc366fd711972",
            "ae7fe1bd456c4784b1a3ec6895d24cda",
            "ce04914242e744bdafe6f58091514cca",
            "eb3c85bfa5e840c4b548e4bf663c3136",
            "a2d75309106a4751950c42718d3d6fa4",
            "58dbdb7d3f1d488a82e48fe0fc50ac48",
            "196b363c9fd94a7c9e3947f6bd9a699f",
            "46c501c6ec1248fc99cf857ecaaf0bfa",
            "be4dcdeb9c744cb7b2ab5e2006365258",
            "e39e954fb7554062b858cc8ce3f48649",
            "552692155caa4bfc80c5d32ef426fdf1",
            "8d5bff34a1dc4ec99f6417debbc21680",
            "b6d19967d6834c14a61e5a0e2705a37d",
            "f0ac549c6d4f490aaee0978c0743f70e"
          ]
        },
        "id": "vrJ64pZDW5ut",
        "outputId": "5b8f154b-3434-4c50-926e-e1d8ac813f7c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5995beface3d4c91a5ddb683eba54357",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5863ca16d58d4fc58fc1d5a881a6ca1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f38a8ea837be4e2eb713ab512312c645",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a1a6a1900754c2a8c8f05ede6c8c95a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb3c85bfa5e840c4b548e4bf663c3136",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu0Ww4XbW701"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = dataframe.label\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.targets[index], dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKvonmB3W-FV"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 320\n",
        "BATCH_SIZE = 16\n",
        "train_dataset = TextDataset(dataframe=test, tokenizer=bert_tokenizer, max_len=MAX_LENGTH)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_dataset = TextDataset(dataframe=df_train, tokenizer=bert_tokenizer, max_len=MAX_LENGTH)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "-Yk-B7msXEJM",
        "outputId": "a7ef219e-c3b3-4fc7-bd4d-9c3ed22d6ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training Loss: 0.42566792384966423, Validation Loss: 0.27410541274398564\n",
            "Epoch: 2, Training Loss: 0.19565956631185194, Validation Loss: 0.30037309251725675\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-13b3c88a05ca>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bert_model.to(device)\n",
        "learning_rate = 2e-05\n",
        "optimizer = torch.optim.AdamW(params=bert_model.parameters(), lr=learning_rate)\n",
        "epochs = 5\n",
        "bert_model.train()\n",
        "import os\n",
        "\n",
        "# Directory where you want to save your models\n",
        "model_save_path = \"/content/drive/MyDrive/planB/bert\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    bert_model.train()\n",
        "    total_loss = 0\n",
        "    # Training loop\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation loop\n",
        "    bert_model.eval()  # Switch to evaluation mode\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_val_loss += loss.item()\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "    # Save model after each epoch\n",
        "    epoch_save_path = os.path.join(model_save_path, f'BERT524_epoch_{epoch+1}')\n",
        "    os.makedirs(epoch_save_path, exist_ok=True)\n",
        "    bert_model.save_pretrained(epoch_save_path)\n",
        "    bert_tokenizer.save_pretrained(epoch_save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEKCxhopgude"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/planB/bert/BERT524_epoch_1')\n",
        "bert_model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/planB/bert/BERT524_epoch_1', num_labels=4)\n",
        "val_dataset = TextDataset(dataframe=df_train, tokenizer=bert_tokenizer, max_len=MAX_LENGTH)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4666bZ6Mg0Zv",
        "outputId": "24feff06-40c1-49fa-f914-2f8060eb0688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9065\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      Class1       0.87      0.87      0.87       500\n",
            "      Class2       0.96      0.99      0.97       800\n",
            "      Class3       0.88      0.88      0.88      1200\n",
            "      Class4       0.91      0.90      0.90      1500\n",
            "\n",
            "    accuracy                           0.91      4000\n",
            "   macro avg       0.91      0.91      0.91      4000\n",
            "weighted avg       0.91      0.91      0.91      4000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 436   18   37    9]\n",
            " [   2  791    5    2]\n",
            " [  17   10 1053  120]\n",
            " [  45    7  102 1346]]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bert_model.to(device)\n",
        "evaluate(bert_model, val_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}